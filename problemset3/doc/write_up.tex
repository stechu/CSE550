\documentclass{article}
\usepackage[pdftex]{graphicx}
\usepackage{wrapfig}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{fullpage}
\usepackage{cite}

\begin{document}

\title{CSE550 Problem Set 3: Performance Evaluation of Least Common Ancestors on Spark}
\author{Vincent Lee, Shumo Chu}
\date{\today}

\maketitle

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Intro
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

The Least Common Ancestor (LCA) algorithm is a method of gauging the distance between two given nodes in a directed acyclic graph (DAG).
In this project we implement the LCA algorithm for a DAG of paper citations; citations graphs are acyclic as paper citations are causal.
Given two papers $p_1$ and $p_2$, we want to find the LCA of the two papers defined $LCA(p_1, p_2)$ if it exists.
A common ancestor is defined as any paper $p_i$ that has a path to both $p_1$ and $p_2$.
Note that the set $S(p_1, p_2)$ denoting the common ancestors of $p_1$ and $p_2$ can be arbitrarily large or empty.
The least common ancestor is defined to be paper $p_{lca}$ in $S(p_1, p_2)$ which minimizes a cost function.
The cost function for any paper $p_i \in S(p_1, p_2)$ is defined as the maximum of the distances between $p_1$ and $p_{lca}$, and $p_2$ and $p_{lca}$ where each edge in the directed graph has equal weight.
Each paper also contains two attributes used to help break ties between equidistant candidates in the graph: a publication year, and unique paper identifier.
Should multiple papers have equal cost by the distance metric, the year of publication is used to break a tie where a lower year of publication minimizes this secondary cost function.
Should multiple papers have both equal distance cost and year of publication, the unique paper identification is used to determine the LCA.
The paper with the least unique identification is chosen as the LCA.

We implement the LCA problem in Spark on top of Amazon's EC2 platform.

\section{Performance Evaluation}

To evaluate the performance and scalability of our implementation, we vary the number of EC2 cluster nodes on our system. 

\subsection{Raw Performance}



TODO: insert graph with computation time scalability with larger dat

TODO: execution time decrease with increase in number of nodes

\subsection{Performance Breakdown}

To analyze the scalability of our implementation and platform, we evaluate the latency of a single query on a varying size dataset and number of cluster nodes.
Due to configuration limitations our experiments were capped at a 19 cluster node maximum (1 master node, 19 slave nodes).

\begin{figure}
\centering
%includegraphics{}
\caption{LCA query latency with increasing data set size}
\label{latency_scaling}
\end{figure}

Figure ~\ref{latency_scaling} shows the query latency time with increasing data set size.
As expected, we see an increase in the query latency with respect to increasing data set size. 
TODO: explain the scaling here

\begin{figure}
\centering
%\includegraphics{}
\caption{LCA query latency with increasing number of cluster nodes}
\label{node_scaling}
\end{figure}

Figure ~\ref{node_scaling} shows the query latency scaling with respect to the number of nodes in the Amazon EC2 cluster instance.
As expected, we see a decrease in query latency time for a fixed data set with more nodes due to additional parallelism.


TODO: why does our performance curve look like it does

We note that the performance of the system is composed of roughly two phases: building the graph and actual graph traversal time.

TODO: if graph building is parallel: For a fixed data size, we expect the graph building time to decrease with the number of nodes in the system as we have implemented a parallel graph building algorithm from the original citation dataset.
TODO: if the graph building is serial: For a fixed data size, the graph building phase will take a fixed amount of time regardless of the number of nodes operating in they system as it is not parallelizable.
Since our primary parallelization optimizations occur in the graph traversal when searching for the LCA, we expect that simply throwing additional cluster nodes at our application will be constrained by Amdahl's limits in terms of  perfspeed up.
This is evident in Figure ~\ref{node_scaling} where we see asymptotic gains in improvement. 
If the performance was entirely composed of entirely parallel computation, we should see an asymptotic performance improvement towards zero latency.
As shown in the performance curve, this is not the case due to serialized portions of the algorithm and other overheads such as network communication and memory access.

The latency scalability we observe is not surprising as Amdahl's law would require we improve the serialized portions of the algorithm in order to achieve truly linear speed up.
However, since our application is not embarrassingly parallel, the scalability numbers we observe from our experiements are fairly reasonable.

TODO: what scalability barriers does our implementation encounter

\subsection{Analytics Platform Evaluation}

The big data analytics platform we used for this project was Apache Spark.
This choice of platform was both a good and bad fit as the LCA algorithm has portions of the algorithm which fall inline well with the Spark API well and some portions do not.
For instance, when performing the graph traversal, it may have been much easier from a programmability and execution model aspect to use GraphLab's optimized single source shortest path API.
However, the Spark execution and programming model allows more flexible parallelization by offering automatically parallelizable arbitrary lambda expressions.
Thus Spark was an acceptable fit for highly optimized graph travesral operations, but a good fit for portions of the algorithm that have no direct GraphLab API mapping such as the JOIN operations we need to execute when computing the cost function of LCA candidates.


TODO: was platform a good fit for analytics application?

TODO: if our analytics suck, why?

\section{Throughput Per Hour}

While query latency is an important metric of performance for many applications, we believe that the LCA algorithm is not latency sensitive; in other words, latency is a complete metric of implementation performance.
One could conceivably build the citation graph, pre-compute all of the LCAs and then cache the results for incoming queries.
This would only require a one time cost of building the index to service arbitrarily many queries; since the LCA index can be built offline, throughput is arguably a more important factor than latency.
Thus to get an idea of how our implementation performs on the throughput axis, we measure the throughput per hour of our system.

We measure the throughput and scalability of our implementation, we tried to determine the number LCAs for a given dataset size N we were able to compute in an hour.

TODO: INSERT THE RESULTS FOR THIS SECTION HERE

TODO: ANALYSIS OF THE DATA HERE


\end{document}
