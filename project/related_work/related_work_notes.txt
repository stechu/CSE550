"Rules of Thumb in Data Engineering" (1999, 2000)

Jim Gray, Prashant Shenoy

Presents an analysis of the trends in memory capacity scaling versus access throughput scaling
Analyzes the storage performance and price correlations
- "extra bit of addressing every 18 months." required to keep up with data usage demand
- this is no longer the case but in 1999/2000 it may have seemed reasonable to assume
- Moore's law pointed out to apply to RAM but also can be extended to processors and disk storage capactiy

Figures documenting disk capacity parameters vs time are presented
- the ratio of disk capacity to disk accesses per second has been "increasing more than 10x per decade"
-> disk accesses are increasingly more precious and scarce resource
-> the number of times data is accessed per byte must decrease in order to amortize this bottleneck
- the proposed solutions so far have been to reduce the number of disk accesses
-> use batching to build one large data transfer as opposed to many small ones
-> use sequential transfers for locality
-> use RAID1 instead of RAID5 - wait really?

Batching transactions and caching reduce random access and write log to disk
Sequential transactions reduce cost of random seek and rotate time
Argument against RAID5 is that it uses many more I/O transactions than the RAID1 version and that since space is free, there's no need to worry about the space overhead of RAID1 (though I don't know if that's true anymore)

Issue of cooling off data access densitities - need to cache things since the number of accesses you get is scaling more poorly than the scaling

"Waht is economical to put on disk today will be economical to put in RAM in about 10 years."

"If you are designing for the next decade, you need build systems that allow one person to manage [1000x the storage capacity of today"

Storage rules of thumb:
1. Moore's Law: Things get 4x better every three years
2. You need an extra bit of addressing every 18 months
3. Storage capacities increase 100x per decade
4. Storage design throughput increases 10x per decade
5. Disk data cools 10x per decade
6. Disk page sizes increase 5x per deacde
7. NearlineTape:OnlineDisk:RAM storage cost ratios are approximately 1:3:300
8. In ten years RAM will cost what disk costs today
9. A person can administer a million dollars of disk storage

Amdahl's system balance rules:
10. Amdahl's parallelism las: If a computation has a serial component S and parallel compoennt P, then the maximum speedup is (S+P)/S
11. Amdahl's balanced system law: A system needs a bit of IO per second for each instruction per second <<<<<<<<<< WE CARE ABOUT THIS ONE
12. Amdahl's memory law: in a balanced system, the MB/MIPS ratio or alpha is 1
13. Amdahl's IO law: programs do one IO per 50,000 instructions

The authors proposed addendums to the system balance rules:
11a. must account for CPI which depends on workload
12a. the alpha ratio is rising from 1 to 4
13a. also depending on workload, IOs per instruction depend on how sequential or random the accesses are

The MIPS measurement must be evalauted with respect to CPI numbers
IOs per instruction are higher for sequential workloads
The alpha ratio is rising from 1 to 2 or 4

Gilder's Laws for Networking
14. deployed bandwidth triples every year
15. link bandwidth improves 4x every 3 years
16. a network message costs 10000 instructiosn plus 10 instructions per byte
17. a disk IO costs 5000 instructions and 0.1 instructions per byte
18. the CPU cost of a system area networking message is 3000 clocks and 1 clock per byte

Caching ratios
19. the 5 minute random rule: cache randomly accessed disk pages that are re-used every 5 minutes (WTF?)
20. the 1 minute sequential rule: cache sequentially accessed disk pages that are re-used within a minute
21. spend 1 byte of RAM to save 1 instruction per second
22. cache web pages if there is any chance they will be re-referenced within their lifetime (cache everything conclusion)

==============================================================================================

Petascale Computational Systems: Balanced CyberInfrastructure in a Data-Centric World
Gordon Bell, Jim Gray, and Alex Szalay

Trends in scientific community:
1. exponential data growth due to Moore's Law
2. flood of data from simulations
3. memory storage is free
4. unprecedented accessibility via Internet

Acquisition, organization, query, and visualization are easily parallelized but statistical analysis, and data mining tend to be non-linear.

Most of the performance focus has been in the TOP-500 rankings which consider CPU performance.

The rules still apply surprisingly enough after 50 years of building systems.

System balance rules when applied to a Peta-operations/second machine:
1 petabyte of RAM
100 terabytes/sec of IO bandwidth and IO fabric
1000000 disk devices @ 100MB/disk to deliver the bandwidth
100000

"Only worth moving data to a remote computing facility if there are more than 100000 CPU cycles per byte of data to perform the analysis."

"For less CPU intensive tasks it is better to co-locate th ecomputation with the data."

Moving data over the Internet is only free if we need to process 100k instructions per byte

Computational problem sizes are heavily tailed according to a Power law distribution

Splitting between Tier 1, 2, and 3 architectures eliminates the compute and I/O impedance mismatches.

An argument against placing all of the investment as one end of the cluster classification (Tier 1) is made as the heavy tailed distribution requires a range of I/O to compute ratios.

"Computational science is changing to be data intensive.
 Funding agencies should support balanced systems, not just CPU farms but also petascale I/O and networking.
 They should allocate resources to support a balanced Tier-1 through Tier-3 cyber infrastructure."
