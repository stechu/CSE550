\documentclass{article}
\usepackage[pdftex]{graphicx}
\usepackage{wrapfig}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{fullpage}
\usepackage{cite}

\begin{document}

\title{CSE550 Project Proposal}
\author{Vincent Lee, Shumo Chu}
\date{\today}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Intro
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

The advent of large distributed systems has enabled unprecedented amounts of computational resources to the end user.
Large data centers today use enormous numbers of commodity servers and routers to operate over massive data sets.
Due to the massive scale of deployment, small software and hardware architectural changes that influence power, network bandwidth, and memory efficiency have enormous impact.
With the rise of big data applications, and dynamically shifting workload patterns, it is imperative that we understand how production workloads on these systems behave in order to determine what aspects of the system work well and what should be changed.
Of particular interest in this proposal are the ratios between compute, network bandwidth, and memory that are necessary to meet the demands of the applications running at scale.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Motivation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Motivation}

Data center architects are primarily concerned with optimizing power consumption and maximizing performance thoughput.
As a result, most research in data center architecture design has been driven by the objective to achieve a so called ``energy proportionality'' where ideally energy usage correlates directly to CPU load. 
While we acknowledge that minimizing energy per FLOP of computation has immediate implications on data center performance, it is a poor metric that fails to capture all of the dynamics present in an active data center.
For instance, data centers have many other metrics that can be measured such as cost per bit sent over the network, or cost per bit moved from main memory or mass storage.
Metrics such as these are not captured in a simple performance per watt calculation.
In addition, measuring the ratios between not only energy and performance, but between network communication bandwidth, memory usage, and compute for various different tasks can have far reaching implications in how we designate the roles between each of these subsystems in data center architectures.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proposal
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Proposal}

We propose analyzing the $googlecluster$ data set to try to infer and quantify the ratios between computation, memory usage, energy, and communication bandwidth. 
While some of this data such as power usage is not readily available, we still believe we can still infer an order of magnitude calculation can still provide insight into how system components should be raesoned about.
To our knowledge, the data set we will analyze contains traces of all tasks running on a ``12k-machine cell over about a month-long period in May 2011''.
The data set contains detailed information for each task on job length, maximum memory usage, disk I/O time, CPI, memory accesses per instruction, and CPU rate which we will use to estimate the afore mentioned metrics.
Although some fields in the dataset are omitted, this is an infrequent occurrence and we plan to simply omit incomplete data fields.
We also note that since the data is captured over a month long period, it will be more meaningful to analyze a sliding window of time in our calculations to determine if there are any cyclic patterns in the data that may provide insight into the data.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Prior Work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Prior Work}

Prior work on the $googlecluster$ data set has involved attempting to characterize and classify application using select fields in the data set and applying k-means clustering techniques ~\cite{clusterdata:Di2013, clusterdata:Mishra2010}.
Similarly, Reiss et al. ~\cite{clusterdata:Reiss2012b} expose numerous insights from the data set including machine workload heterogeneity, erratic and dynamic resource availability, poor scheduling predictability, and special resource constraint bottlenecks.
The authors in both ~\cite{clusterdata:Liu2012, clusterdata:Mishra2010} perform similar workload characterization analysis to expose insight in scheduling behaviors and .
In ~\cite{clusterdata:Lui2012}, the authors in partiular expose the criticality of scheduling constraints and resource allocation in the behavior of the workloads.
In ~\cite{clusterdata:Di2012a} Di et al. show that based on an analysis of the trace Google data centers show better resource allocation granularity to their Grid/HPC counterpart systems.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Deliverables
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Deliverables}

We plan to deliver a comprehensive analysis of the ratios between computation, memory, and network bandwidth to provide insight into how data center architecture should or should not designed.
Using this data, we plan to analyze any interesting trends and perhaps suggest architectural design modifications that could alleviate bottlenecks or problems we may observe.
We also plan to perform a very rough estimate of the energy usage to determine rough energy per FLOP and byte ratios.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{proposal.bib}

\bibliographystyle{IEEEtran}

\end{document}
