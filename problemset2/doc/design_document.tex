\documentclass{article}
\usepackage[pdftex]{graphicx}
\usepackage{wrapfig}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{fullpage}
\usepackage{cite}

\begin{document}

\title{CSE550 Problem Set 2: Paxos Design Document}
\author{Vincent Lee, Shumo Chu}
\date{\today}

\maketitle

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Intro
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

In this document, we outlined our software implementation of the Paxos consensus protocol explained in TODO: ADD REFERENCE.
We present a high level software architecture, justify our design decisions, and explain any simplfying assumptions that we make to our design.
We then explain how to use our implementation, and any outstanding problems with our implementation.
A quick discussion of any interesting tidbits we encounter concludes the document.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Paxos Protocol Summary
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Paxos Protocol}

The Paxos protocol originally proposed by Lamport in TODO: ADD REFERENCE is a consensus protocol designed to make forward progress even in the face of arbitrary failures.
We interpret consensus to be defined as the following constraints:

\begin{enumerate}
\item One and only one value may be chosen per Paxos instance
\item The value must come from the set of proposals
\item Only the chosen values can propagate to learners
\end{enumerate}

The Paxos protocol consists of three classes of members: proposers, acceptors, and learners.
\begin{itemize}
\item Proposers are defined as entities that present proposals to acceptors.
\item Acceptors are defined as entities that are responsible for arbitrating whether a proposal is accepted or not.
\item Learners are defined as entities that are subscribed to a particular value that a consensus determines.
\end{itemize}

In any Paxos instance, in order to tolerate node failure, multiple proposers and acceptors participate in a Paxos instance.
A Paxos instance is resolved by the following a two phase protocol.

\subsection{Paxos Phase I}

In the first phase of the protocol, proposers create proposals with some associated monotonically increasing sequence number N and some instance number I. 
The sequence number is always larger than any previous sequence number the proposer has issued. 
A propose message is then sent to the set of acceptors with the sequence number chosen. 
When an acceptor receives a propose message, it compares the sequence number of the proposal to all previous sequence numbers it has seen.
If the sequence number received is less than any sequence number the acceptor has seen, the proposal is rejected and ignored.
A response message with the highest proposal seen and the value accepted is sent to the proposer instead.

If the sequence number received by the acceptor is greater than or equal to the prior sequence numbers observed by the acceptor, it ``promises'' to reject all messages less than the sequence number received from that point forward.
A response is issued with this ``promise'' to the original proposer.
Meanwhile, the original proposer wait until it receives responses from a majority of the acceptors it initially issued proposals to with sequence number N.
There are two cases at this stage of the protocol: the proposer gets messages from a majority of acceptors with a prepare acknowledgement for it's proposal number, 
or the proposer gets messages from a majority of acceptors with a prepare not acknowledgement (prepare NACK), the highest value each acceptor has accepted, and the value associated with this highest proposal number.
If the proposer gets a majority of previously accepted values from the acceptors it means that the instance may have already been resolved.
At this point the proposer will abandon its original proposal and propose this new value it received from a majority of acceptors and the proposal number.

\subsection{Paxos Phase II}

If a majority of messages are prepare NACKs with the same previously accepted value, the proposer changes the value it wants to propose to that value and the proposal number to the number in the prepare NACK message.
Once this resolution occurs, the proposer issues an accept request with the proposal number and value.
An acceptor will accept the proposal and chosen value if the highest sequence number is sees is still N, and reject the message otherwise.
Upon successful receipt of the accept message, messages are issued to all relevant learners with the value chosen and an accept message is issued to the original proposer.
If an acceptor rejects the accept request, the message will be discarded and no response will be issued.
A proposer knows its attempt to chose a value has failed if it does not receive responses from a majority of acceptors.

If at the end of a single iteration of this protocol, no value is ultimately chosen, the procedure is repeated with a higher sequence number of each node until a value is eventually chosen.
If the proposer is able to get a majority of accept requests, it knows that the Paxos instance has been resolved and moves on to the next one.
If during the first phase, the proposer had to abandon the original value, it tries to propose this value again in the next Paxos instance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Implementation Details
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Paxos Implementation of Lock Service}

Our design leverages the Paxos algorithm to implement a the ordering of commands for a simple lock service.
Issuable commands in this service are $lock(x)$ and $unlock(x)$ where $x$ is an integer.
The end goal of this service is to provide consensus upon a series of commands in which locks are resolved in a valid order.
The state machine in this service is defined to be the state of the locks and is replicated across servers for resilience to failure.
Each instance of the Paxos protocol resolves exactly one command to be issued for that instance.
Instances are numbered in monotonically increasing order and correlate to sequence in which the $lock(x)$ and $unlock(x)$ commands are issued.

\subsection{Simplifying Assumptions}

We assume the same assumptions about the distributed system model as originally proposed in the Paxos paper. Namely, we assume the following:

\begin{enumerate}
\item No Byzantine failures will occur.
\item All entities in the distributed system operate asynchronously at arbitrary speed
\item Any node that fails in the system will observe fail-stop behavior
\item Messages at any phase during the protocol can be dropped, reordered, or duplicated
\end{enumerate}

We also make the following assumptions to simplify our implementation of the Paxos algorithm:

\begin{enumerate}
\item Any nodes that experience a failure will permanently be removed from the Paxos group and will not attempt recovery and restart
\item Clients to the system will not intentionally behave maliciously
\item Entities will not attempt to retransmit messages in the case of message loss or timeout
\item Clients know which entities are part of the Paxos group
\item The client-server communication channel is reliable
\item Client requests made to a server are serviced one at a time - i.e. does not handle multiple concurrent client requests
\end{enumerate}

Our implementation initializes each server with a different proposal number. 
When proposing higher numbered proposals, the initial proposal number is incremented by the number of servers initially in the system, not the active number of servers in the Paxos group.
This is to ensure strict partitioning of the proposals.
As a consequence, we do not implement leader based Paxos since the possibility of a livelock situation is negligbly small.

\subsection{Failure Mode Considerations}

The following is a list of failure modes that our implementation of the lock service considers:

\begin{enumerate}
\item Upon a connection by the client, the Paxos member servicing the client requests promptly fails. 
  We resolve this by tagging client requests with a unique identifier.
  The unique identifier is used upon reconnecting to a new Paxos member to validate whether requests by the failed Paxos member were accepted.
  If a command with the unique identifier appears in a previous resolution to an instance of Paxos, we know that the command has already been accepted and do not need to attempt to propose it again.
  If the command identifier does not appear in the list of previously resolved Paxos instances, we know that the original proposal failed and must be proposed again.
\item Upon acquiring a lock, the server node dies and the lock is orphaned.
  We consider allowing the client to reconnect to a different server node to complete execution of the lock sequence to free the lock.
  Clients in this scenario issue a client identifier tag with each request allowing the server to authenticate if the client was the original holder of the lock.
\end{enumerate}

\subsection{High Level Software Architecture}

We first note that it is necessary for each Paxos process to act as a proposer, acceptor, and learner as suggested by Lamport in ``Paxos Made Simple''.
This is because there are multiple instances of Paxos that may be running and so new proposals must be attributed to instance numbers that have not yet been resolved.
Proposers and learners in particular must reside on the same process because only learners are defined to learn the outcome on a Paxos instance.
A proposer must know which instances of Paxos have been resolved so that it may make a proposal for an instance that has not been resolved and therefore must also be a learner.
This architecture also eliminates the possibility that proposer's receive an accept request acknowledgement while learners may not due to catastrophic message losses.
By making a process both an acceptor and a learner, it is guaranteed that both the acceptor and learner get a majority response or that both fail to get a majority response.

\subsection{Implementation Details}

\subsubsection{Server Initialization}

We use a script to initialize each of our servers and bring our Paxos group online.
Each server is initialized with a unique server identifier and a list of hostname and ports for all other servers in the Paxos group to establish communication channels.
This is also to allow each server to independently compute a mutually indepedent set of monotonically increasing proposals numbers and avoid proposal number collisions.
These server connection channels are used to communicate messages between Paxos members and is an all-to-all network.
To ensure safe bring up, we first initialize listening sockets on each server, then initialize the connections betwen servers.
Each server in addition to inter-server communication channel sockets, exposes a client facing server socket which are used exclusively by clients to connect to.
This architecture decision was to simplify the message handling and control complexity that would have gone in to processing both client state and Paxos member states during the protocol.
To avoid port collision, listening socket numbers on each server are predefined.

\subsubsection{Message Passing}

A message data structure is used to contain the contents of any message traveling to and between the Paxos group members.
The message type determines how the message should be interpreted; message types are: PREPARE, PREPARE\_ACK, ACCEPT, ACCEPT\_ACK, CLIENT, CLIENT\_ACK.
A explanation of each message type is below:
\begin{itemize}
\item PREPARE - indicates message is a prepare request from a proposer; holds the proposal number, instance number, and the origin server
\item PREPARE\_ACK - indicates message ia an acknowledgement or promise message for a proposer from an acceptor; holds a proposal number, and instance number
\item ACCEPT - an accept request message; holds a proposal number, instance number, value, and origin
\item ACCEPT\_ACK - an accept acknowledgement; sends proposal number, instance number, and value
\item CLIENT - indicates message came from client with a command request; contains value
\item CLIENT\_ACK - an acknowledgement message send to the client once command proposed has been accepted
\end{itemize}

Each server runs two communication server channels.
One server socket is dedicated to receiving and connecting to exclusively other servers
A second server socket is dedicated to listening to client connections on the proposer process discussed later.
To avoid port collisions, each server assumes that the port number supplied during initialization and the port number plus 1 are both allocated for that server.

\begin{figure}
\centering
\includegraphics[width=6in]{paxos_group_connections.jpg}
\caption{Paxos Group Connections - each Paxos group member holds connections to clients and connections to communication to every other server in the Paxos group} 
\end{figure}

Once a command has been successfully added to an instance of Paxos, an acknowledgement message is sent to the client notifying the client that the lock was acquired. The connection then waits until a new command is issued by the client and repeats the processing or the client terminates the connection.

\subsection{Process Architecture}

A server contains one instance of each Paxos role: a proposer, an acceptor, and a learner.
In our implementation, to reduce complexity, we make the proposer also a learner and do not have separate learner entities operating asynchronously in our design.
The proposer, and acceptor run on separate processes for each instance of a server in order to operate asynchronously.
The proposer and learner each have a message queue which is fed with in-bound messages from other servers and client requests.
These message queues are used to serialize the message streams that may be coming from other servers.
Each connection to a Paxos member by another server first connection to a Listening process.
This Listening process spawns a new connection process to handle all requests for the new connection.
Each of these connection processes handle inter-server communication messages and puts them on the appropriate message queue bound for the Proposer process, or Acceptor process on that server.
The proposer process is responsible for opening a client facing socket which allows clients to connect to the service and send request messages.
Proposers handle each connection in a blocking manner where the client connection will block until the requested command issued by the client makes it into an instance of Paxos.
The process architecture in our implementation is shown in Figure ~\ref{paxos_member}.

\begin{figure}
\centering
\includegraphics[width=4in]{paxos_member_architecture.jpg}
\caption{Process Architecture - message queues to learner, proposer, and acceptor are serialized and each put() on the message queue is atomic}
\label{paxos_member}
\end{figure}

\subsection{Proposer Finite State Machine}

The proposer executes a finite state machine as summarized in Figure ~\ref{proposer_fsm}.

\begin{figure}
\centering
\includegraphics[width=6in]{proposer_fsm.jpg}
\caption{Proposer Finite State Machine}
\label{proposer_fsm}
\end{figure}

State definitions:
\begin{itemize}
\item[IDLE] PROPOSER is waiting for client connection to issue a COMMAND to propose. 
  Upon receiving a COMMAND to propose, a transition to READY is made.
\item[READY] PROPOSER has received a message but no issue proposals to each ACCEPTOR in the Paxos group.
  In this state, the PROPOSER issues PREPARE messages to each ACCEPTOR and unconditionally transitions to the PROPPOSING state.
\item[PROPOSING] PROPOSER has issued PREPARE messages and is waiting for a majority of ACCEPTORS to response with a PREPARE ACK message.
  In this state, if the PROPOSER does not receive a sufficient number of PREPARE ACK messages for its PREPARE request it transitions to the READY state and attempts another proposal.
  If the PROPOSER gets a majority of responses, move to the ACCEPT state.
\item[ACCEPT] PROPOSER has received a majority for proposal and issues an ACCEPT message with the COMMAND to each ACCEPTOR in the Paxos group. 
  An unconditional transition to the ACCEPTING state occurs.
\item[ACCEPTING] PROPOSER has issued ACCEPT requests. 
  If an ACCEPT ACK message is received from a majority of ACCEPTORS, consider the proposal COMMAND chosen and advance to BROADCAST state.
 If an insufficient number of ACCEPT ACK message are received after a given time, assume the ACCEPT attempt failed and transition to the READY state to attempt another round. 
\item[BROADCAST] PROPOSER issues LEARNER messages to all Paxos learner members in the system notifying that the COMMAND for the INSTANCE has been chosen.
\end{itemize}

\subsection{Lock Service Implementation}

The client is implemented such that after sending the server a command message with type CLIENT to request a command be proposed, it will block on a receive from the server until it receives a CLIENT ACK message.
If the server receives a CLIENT message, it will block until it has confirmed that the Paxos group has accepted the proposed command into an instance in the Paxos group and that the appropriate lock was acquired or released.
Each client issues a unique client\_id field in order to distringuish its requests from other clients which may issue similar requests.

Because the service for each client is blocking until the request is resolved, in the event of lock contention a client connection will simply wait until lock request has been processed.

In order to ensure that multiple requests for the same lock do not simultaneously get assign in subsequent instances of Paxos, we impose conservative constraints on the conditions for which the instance number may be incremented.
A Paxos proposer may only advance it's instance number to some instance value N if it has learned the values of all instances less than N.
If a value for instance N-1 is not resolved, a proposer must attempt to learn it's value by submitting a proposal to that instance.

Though this restriction means that only one instance of Paxos may be resolved at a time, it imposes the guarantee that we will never acquire the same lock simultaneously for two adjacent instances of Paxos (ex. instance N = lock(5), instance N+1 = lock(5)).

The correct locking service behavior then follows as proposers will necessarily have the correct view of the lock state up to instance N before trying to acquire a lock that is currently held.
If a proposer sees that the lock it wants is currently held, it will wait until the lock is released in a future instance before attempting to acquire it.

Once the requested operation finished, the server issues a message with type CLIENT ACK indicating to the client that the Paxos group successfully accepted the proposal.

The client can then either disconnect from the server, or issue another command to be processed.
Unless the server instance dies or is shutdown, the server never closes the client connection.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Instructions for Use
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{How to Use Our Implementation}

All of our code can be found in the $\${ROOT_DIR}/src$ folder.
The folder contains a number of unit test files which exercise different aspects of the design.
Each unit test $unittest$ can be executed by simply running $./unittest$ in the $src$ directory.

Unit test files:
\begin{itemize}
\item [acceptor\_test.py] Tests if the client is able to send messages to a dummy server instance, and correctly processes the acknowledgement messages from the server. 
  The dummy server is configured to simply loopback the client request message as an acknowledgement.
\item [acceptor\_test.py] Exercises the acceptor process logic. 
  Creates a dummy server instance for the Paxos acceptor to connect to and injects a sequence of messages into the acceptor process. 
  Response messages are validated and replied to where expected.
\item [proposer\_test.py] Exercises the proposer process logic.
  Creates a dummy server instane for the Paxos proposer to connect to and injects message sequnces to exercise the FSM.
  Response messages are validated and replied to where necessary.
\item [integration\_test.py] Tests basic bring up of a Paxos group with no failures.
  Clients are connected to server instances to verify use cases under the absence of node failures.
\item [failure\_test.py] Tests if the Paxos group still operates correctly under the presence of node failures.
  Server nodes are brought down by terminating all process associated with that server node.
\end{itemize}

To run all tests, in the $src$ directory, issue $make test$.
This should fire off each test case.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Known Issues
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Known Issues}

STUB

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Discussion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion}

In this section, we discuss a series of potential problems with our implementation of the lock service which we do not resolve, but would have been important considerations should this have been intended to be a production level system.
We also present discussion of

\subsection{Resolving Deadlock}

In our implementation of the lock service, it is entirely possible for deadlock to occur.
The canoncial deadlock scenario is if client A requests locks 1 and 2, while client B requests lock 2 and 1.
If client A is able to acquire lock 1 and client B is able to acquire lock 2, then the system deadlocks.
This particular deadlock stems from a poorly designed lock application and thus we punt this problem to the client application programmer to avoid this sort of deadlock issue.

A second deadlock can occur in the case of orphaned locks.
A lock can be orphaned if a lock command was issued, and the server node failed before it could issue the unlock command associated with the client.
Since each lock acquisition is tagged with a client id, unless the original client comes back online at another server node, no other client may release this lock.
As a result, any other client requesting the orphaned lock is deadlocked.

In order to resolve this issue, we would have needed to implement some sort of watchdog timeout which after a certain period of time proposes to unlock the lock on behalf of the dead client.
This would eventually free up the lock but also does not bypass the Paxos consensus protocol by going through established means to release the lock.

\subsection{Proposal Priority}

In our current implementation, upon initialization, each server is given a unique identifier.
This identifier is made the server's initial proposal number such that proposals from different servers avoid collisions.
However, arbitrarily initializing the proposal number to the server number gives the highest numbered server an unfair advantage.
Thus in the case where all Paxos members are starting a fresh instance simultaneously, the highest numbered proposing server would always ``win'' in that instance.
Since we do not shuffle the initial proposal values after each instance, the highest numbered server will always enjoy a slight advantage over its peers.
To fix this, we would have had to implement a variant of Paxos where a leader token would be rotated among Paxos members such that each member would enjoy highest priority the same number of instances as every other proposer.

\end{document}
